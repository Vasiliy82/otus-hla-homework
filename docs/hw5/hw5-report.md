Масштабируемая подсистема диалогов
Цель:
В результате выполнения ДЗ вы создадите базовый скелет микросервиса, который будет развиваться в дальнейших ДЗ.  
В данном задании тренируются навыки:
- декомпозиции предметной области;
- построения элементарной архитектуры проекта&
Описание/Пошаговая инструкция выполнения домашнего задания:
**Реализовать функционал:**
- Отправка сообщения пользователю (метод `/dialog/{user_id}/send` из [спецификации](https://github.com/OtusTeam/highload/blob/master/homework/openapi.json "спецификации"))
- Получение диалога между двумя пользователями (метод `/dialog/{user_id}/list` из [спецификации](https://github.com/OtusTeam/highload/blob/master/homework/openapi.json "спецификации"))
---
** Требования**
- Обеспечить горизонтальное масштабирование хранилищ на запись с помощью шардинга.
- Предусмотреть:
- Возможность решардинга
- (опционально) “Эффект Леди Гаги” (один пользователь пишет сильно больше среднего)
- Наиболее эффективную схему.
---
**Форма сдачи ДЗ**
- Предоставить ссылку на исходный код (github, gitlab, etc)
- Предоставить докеризированное приложение, которое можно запустить при помощи docker-compose (может лежать рядом с исходным кодом) ИЛИ развернутое приложение, доступное извне ИЛИ инструкция по запуску
Критерии оценки:

Оценка происходит по принципу зачет/незачет.  
Требования:

1. (опционально) Верно выбран ключ шардирования с учетом "эффекта Леди Гаги".
2. В отчете описан процесс решардинга без даунтайма.

Компетенции:
- Работа с высоконагруженными БД
    - - осуществлять настройку шардирования с Citus

## Архитектурная проработка

### Введение в проблематику

В процессе развития нашего учебного проекта, изначально построенного в виде монолитного приложения, мы получили задачу о переходе к микросервисной архитектуре. Первым шагом стало выделение функциональности работы с диалогами пользователей в отдельный микросервис. Перед нами встала задача интеграции нового микросервиса с авторизацией на основе JWT, поскольку этот функциональный блок реализован в монолите.

В текущем решении монолит отвечает за следующие аспекты авторизации:

1. Генерацию JWT токенов.
2. Проверку подписи JWT, чтобы удостовериться в его подлинности.
3. Проверку токенов по черному списку (blacklisted tokens), который используется для управления отзывом токенов.

При выделении микросервиса важно минимизировать дублирование кода и функциональности, а также предотвратить возможные проблемы конкурентного доступа к данным черного списка между монолитом и микросервисом. Кроме того, необходимо учесть требования к производительности и надежности, так как система должна работать в режиме 24/7 и предъявляет высокие требования к времени отклика.

В связи с этим возникает ряд вопросов:

- Как обеспечить корректную проверку JWT в микросервисе, избегая дублирования логики монолита?
- Как управлять черным списком токенов, исключая конкурентный доступ из двух систем?
- Каким образом минимизировать задержки и зависимость микросервиса от монолита?

Ниже рассмотрены несколько архитектурных решений для данной задачи, их преимущества и недостатки, а также предоставлены рекомендации по выбору подхода, наиболее соответствующего нашим требованиям.

### Предложенные варианты
#### Вариант 1: Проксирование запросов через монолит

- **Идея:** Микросервис делегирует проверку JWT монолиту.
    
- **Как работает:**
    
    1. Микросервис перенаправляет запрос на монолит с токеном.
    2. Монолит проверяет подпись JWT, а также сверяет токен с черным списком.
    3. Монолит возвращает результат проверки микросервису.
- **Преимущества:**
    
    - Централизованная проверка токенов.
    - Нет дублирования кода авторизации.
    - Микросервис изолирован от логики работы с JWT и черным списком.
- **Недостатки:**
    
    - Увеличение латентности (дополнительный запрос к монолиту).
    - Высокая зависимость микросервиса от доступности монолита.
    - Сложности масштабирования.

---

### Вариант 2: Выделение авторизации в отдельный микросервис

- **Идея:** Создать централизованный сервис авторизации, отвечающий за раздачу и проверку JWT.
    
- **Как работает:**
    
    1. Все приложения (монолит и микросервис) обращаются к этому сервису для проверки токенов.
    2. Сервис авторизации управляет черным списком токенов.
- **Преимущества:**
    
    - Централизованная логика проверки JWT и работы с черным списком.
    - Независимость микросервиса от монолита.
    - Улучшенная масштабируемость за счет выделенной ответственности.
- **Недостатки:**
    
    - Требуется разработка и сопровождение нового сервиса.
    - Переход монолита на взаимодействие с сервисом авторизации может потребовать рефакторинга.

---

### Вариант 3: Репликация данных черного списка

- **Идея:** Черный список токенов реплицируется из монолита в микросервис.
    
- **Как работает:**
    
    1. Монолит периодически (или в режиме реального времени) синхронизирует данные черного списка с микросервисом.
    2. Микросервис использует локальную копию данных для проверки токенов.
- **Преимущества:**
    
    - Микросервис работает автономно и не зависит от монолита.
    - Минимальная латентность проверки JWT.
- **Недостатки:**
    
    - Сложность синхронизации данных.
    - Возможны временные рассогласования между черными списками монолита и микросервиса.
### Вариант 4: Использование Message Queue для черного списка

- **Идея:** Монолит публикует изменения в черном списке через очередь сообщений, а микросервис подписывается на эти события.
    
- **Как работает:**
    
    1. При добавлении токена в черный список монолит публикует событие в очередь сообщений (например, RabbitMQ, Kafka).
    2. Микросервис обновляет локальный черный список на основании этих событий.
    3. Проверка JWT выполняется микросервисом с учетом актуального черного списка.
- **Преимущества:**
    
    - Асинхронная и масштабируемая архитектура.
    - Микросервис автономен в проверке токенов.
    - Нет прямого конкурентного доступа к базе данных черного списка.
- **Недостатки:**
    
    - Усложнение архитектуры (необходима настройка Message Queue).
    - Возможна задержка между публикацией и обновлением локального черного списка.

---

### Вариант 5: Проверка подписи JWT + запрос на черный список

- **Идея:** Микросервис проверяет подпись JWT локально, а черный список запрашивает у монолита.
    
- **Как работает:**
    
    1. Микросервис проверяет подпись JWT локально.
    2. При необходимости микросервис делает запрос к монолиту для проверки токена в черном списке.
- **Преимущества:**
    
    - Минимальное дублирование функций.
    - Снижается нагрузка на монолит (проверка подписи выполняется локально).
- **Недостатки:**
    
    - Зависимость от доступности монолита.
    - Дополнительная латентность на запрос к монолиту.

### Выбранный вариант

Для high load приложения выбран подход, при котором микросервис делегирует проверку авторизации монолиту. Монолит выполняет проверку JWT (подпись и черный список) и передает микросервису идентификатор пользователя через заголовок `X-User-Id`. Это решение минимизирует нагрузку на микросервис, устраняет необходимость повторной проверки токена и позволяет сосредоточиться на бизнес-логике.
**Обоснование:**
1. **Избежание двойной проверки:** Проверка подписи и черного списка JWT остается на монолите, что снижает накладные расходы в микросервисе.
2. **Простота реализации:** Микросервис работает с готовым идентификатором, не заботясь о деталях JWT.
3. **Централизация авторизации:** Все проверки выполняются в одном месте, что гарантирует консистентность и упрощает управление.
4. **Оптимизация для нагрузки:** Минимизация обработки токенов в микросервисе позволяет улучшить производительность.
5. **Сложность проверки полномочий:** Проверка полномочий на создание сообщения (например, наличие дружбы между пользователями) тесно связана с данными монолита. Поскольку данные о дружбе остаются в монолите и их перенос в микросервис диалогов не планируется, выполнение этой проверки в монолите логично и упрощает архитектуру.
6. **Гибкость:** В будущем проверку можно вынести на уровень сетевого слоя (например, с использованием Service Mesh).

## Реализация

[Ссылка на коммит](https://github.com/Vasiliy82/otus-hla-homework/commit/ea7e303c1e96e8f53d67431553769127df948300)
**Проект:** `backend-messages`  
**Цель:** Реализация микросервиса для масштабируемой подсистемы диалогов, обеспечивающего горизонтальное масштабирование хранилищ на запись.
### 1. Реализованный функционал

#### 1.1. Отправка сообщения пользователю

- **Метод:** `POST /dialog/:partnerId/send`
- **Описание:**  
    Пользователь отправляет сообщение другому пользователю, указав его ID (`partnerId`).  
    Реализация включает:
    - Валидацию входных данных (текст сообщения).
    - Извлечение ID отправителя из контекста HTTP (заголовок X-User-Id).
    - Вызов метода `SendMessage` сервиса для сохранения сообщения.

#### 1.2. Получение диалога между двумя пользователями

- **Метод:** `GET /dialog/:partnerId/list`
- **Описание:**  
    Пользователь может запросить историю сообщений с другим пользователем. Реализована пагинация с использованием `offset` и `limit` в запросах.  
    Логика:
    - Извлечение ID отправителя из контекста HTTP (заголовок X-User-Id).
    - Вызов метода `GetDialog` сервиса, который получает данные из репозитория.
    - Формирование ответа в формате JSON.

### 2. Архитектура проекта

#### 2.1. Обработчики REST API (HTTP)

- **Файл:** [`internal/rest/rest.go`](https://github.com/Vasiliy82/otus-hla-homework/blob/ea7e303c1e96e8f53d67431553769127df948300/backend-messages/internal/rest/rest.go)
- **Реализация:**
    - Обработчики запросов:
        - `SendMessage` для отправки сообщений.
        - `GetDialog` для получения списка сообщений.
    - Используется Echo для работы с HTTP-запросами.
    - Логирование ошибок и цепочек вызовов.
    - Валидация входных параметров (например, проверка `offset` и `limit`).

#### 2.2. Сервисный слой

- **Файл:** [`internal/services/services.go`](https://github.com/Vasiliy82/otus-hla-homework/blob/ea7e303c1e96e8f53d67431553769127df948300/backend-messages/internal/services/services.go)
- **Описание:**
    - Логика работы с диалогами изолирована в сервисе.
    - Методы:
        - `SendMessage`: сохраняет сообщение в репозитории.
        - `GetDialog`: получает сообщения между пользователями, обрабатывает пагинацию.

#### 2.3. Репозиторий

- **Файлы:**
    - Интерфейс: [`internal/domain/interfaces.go`](https://github.com/Vasiliy82/otus-hla-homework/blob/ea7e303c1e96e8f53d67431553769127df948300/backend-messages/internal/domain/interfaces.go)
    - Реализация: [`internal/infrastructure/postgresqldb/postgresqldb.go`](https://github.com/Vasiliy82/otus-hla-homework/blob/ea7e303c1e96e8f53d67431553769127df948300/backend-messages/internal/infrastructure/postgresqldb/postgresqldb.go)
- **Описание:**
    - Методы:
        - `SaveMessage`: сохраняет сообщение в базу данных.
        - `GetMessages`: получает сообщения с учетом `offset` и `limit`.
    - Используется CitusDB для шардинга.

#### 2.4. Middleware

- **Файлы:**
    - [`internal/middleware/cors.go`](https://github.com/Vasiliy82/otus-hla-homework/blob/ea7e303c1e96e8f53d67431553769127df948300/backend-messages/internal/middleware/cors.go)
    - [`internal/middleware/timeout.go`](https://github.com/Vasiliy82/otus-hla-homework/blob/ea7e303c1e96e8f53d67431553769127df948300/backend-messages/internal/middleware/timeout.go)
- **Описание:**
    - Middleware для обработки CORS, позволяя делать запросы с разных источников.
    - Middleware для установки таймаута на выполнение запросов.

#### 2.5. Конфигурация

- **Файл:** [`app-messages.yaml`](https://github.com/Vasiliy82/otus-hla-homework/blob/ea7e303c1e96e8f53d67431553769127df948300/backend-messages/app-messages.yaml)
- **Содержит:**
    - Настройки подключения к базе данных (PostgreSQL + Citus).
    - Настройки API, включая таймауты запросов и адрес сервера.


### 3. База данных и инфраструктура

#### 3.1. База данных

- **Технология:** CitusDB (PostgreSQL с поддержкой шардинга).
- **Шардирование:**
    - Используется таблица с шардированием по ID пользователя или ID диалога. Это позволяет эффективно обрабатывать сценарии с высокой нагрузкой.

#### 3.2. Инфраструктура

- **Файл:** [`environments/hw5/compose.yml`](https://github.com/Vasiliy82/otus-hla-homework/blob/ea7e303c1e96e8f53d67431553769127df948300/backend-messages/environments/hw5/compose.yml)
- **Описание:**
    - Кластер Citus с одним мастером и тремя воркерами.
    - Конфигурация поддерживает добавление новых воркеров для решардинга без даунтайма.
    - Включен контейнер `goose` для управления миграциями.



### 4. Ключевые аспекты отказоустойчивости и масштабируемости
#### 4.1. Горизонтальное масштабирование
- **Настройка базы данных:**  
    Система настроена для горизонтального масштабирования с использованием Citus, расширения PostgreSQL, которое превращает его в распределённую базу данных.
    - **Шардирование сообщений:**  
    Сообщения распределяются по ключу диалога. Это обеспечивает равномерное распределение нагрузки между шардами и эффективно компенсирует эффект "Леди Гаги", избегая концентрации активности на одной шарде.
#### 4.2. Решардинг
- **Добавление новых воркеров:**  
    Для увеличения вычислительных мощностей в кластер можно добавлять новые узлы-воркеры с помощью функции `citus_add_node`. После добавления воркера данные можно перераспределить между узлами для достижения равномерной нагрузки.
- **Перераспределение шардов:**  
    Используя функцию `citus_rebalance_start`, можно инициировать перераспределение шардов между узлами. Это выполняется без остановки работы системы, что минимизирует даунтайм.
#### 4.3. Резервирование
- **Резервирование данных:**  
    Citus поддерживает репликацию данных. Для каждого шарда можно создать резервные копии на разных узлах. Это обеспечивает отказоустойчивость и позволяет быстро восстановить данные при сбое одного из воркеров.  
- **Настройка репликации:**  
    Количество реплик для каждого шарда настраивается через параметры конфигурации. Это позволяет балансировать между производительностью и уровнем отказоустойчивости.
- **Отказоустойчивость координатора:**  
    Координатор в Citus является точкой входа в кластер и обрабатывает запросы клиентов. Чтобы обеспечить его отказоустойчивость:
    - Используется кластерная система с поддержкой автоматического переключения (например, Patroni или PgBouncer).
    - Можно настроить репликацию координатора с использованием PostgreSQL Streaming Replication.
    В случае сбоя основного координатора реплика становится основным узлом, а клиенты автоматически перенаправляются на него. Это гарантирует непрерывную работу сервиса.

### 4. Ключевые аспекты отказоустойчивости и масштабируемости

#### 4.1. Горизонтальное масштабирование

- База данных настроена для горизонтального масштабирования с помощью CitusDB.
- Сообщения шардуются по ключу диалога, что позволяет компенсировать эффект "Леди Гаги": диалоги активного пользователя не попадут на одну шарду, а будут равномерно распределяться.

#### 4.2. Решардинг

- Процедура добавления новых воркеров в кластер описана [в документации](https://docs.citusdata.com/en/stable/admin_guide/cluster_management.html#add-a-worker).
- При добавлении воркеров данные перераспределяются автоматически, обеспечивая равномерную нагрузку.

### 5. Пример запросов

#### 5.1. Отправка сообщения

```bash
curl -X POST "http://localhost:8080/dialog/4720fe78-2f95-45ef-a14b-028f9c110d9d/send" \
-H "Content-Type: application/json" \
-H "X-User-Id: 123e4567-e89b-12d3-a456-426614174000" \
-d '{"text":"Hello!"}'

curl -X POST http://localhost:8080/dialog/123e4567-e89b-12d3-a456-426614174000/send \
-H "Content-Type: application/json" \
-H "X-User-Id: 4720fe78-2f95-45ef-a14b-028f9c110d9d" \
-d '{"text":"Hi!"}'
```

#### 5.2. Получение диалога

```bash
curl "http://localhost:8080/dialog/4720fe78-2f95-45ef-a14b-028f9c110d9d/list?offset=0&limit=10" \
-H "X-User-Id: 123e4567-e89b-12d3-a456-426614174000"

curl "http://localhost:8080/dialog/123e4567-e89b-12d3-a456-426614174000/list?offset=0&limit=10" \
-H "X-User-Id: 4720fe78-2f95-45ef-a14b-028f9c110d9d"


```

### 6. Итоги

- Реализован базовый микросервис для работы с диалогами.
- Обеспечена возможность масштабирования хранилища данных через шардинг и добавление новых воркеров.
- Предусмотрен функционал решардинга без даунтайма.
- Сервис поддерживает высоконагруженные сценарии благодаря шардингу и грамотной архитектуре.


